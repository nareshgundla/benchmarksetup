<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
--><configuration>
  <!-- WARNING!!! This file is auto generated for documentation purposes ONLY! -->
  <!-- WARNING!!! Any changes you make to this file will be ignored by Hive.   -->
  <!-- WARNING!!! You must make your changes in hive-site.xml instead.         -->
  <!-- Hive Execution Parameters -->
  <property>
    <name>hive.spark.client.connect.timeout</name>
    <value>3600s</value>
  </property>
  <property>
    <name>hive.spark.client.server.connect.timeout</name>
    <value>3600s</value>
  </property>
  <property>
    <name>hive.execution.engine</name>
    <value>spark</value>
    <description>
      Expects one of [mr, tez, spark].
      Chooses execution engine. Options are: mr (Map reduce, default), tez (hadoop 2 only), spark
    </description>
  </property>
  <property>
    <name>mapreduce.input.fileinputformat.split.maxsize</name>
    <value>750000000</value>
  </property>
  <property>
    <name>hive.vectorized.execution.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.exec.reducers.bytes.per.reducer</name>
    <value>67108864</value>
    <description>size per reducer.The default is 256Mb, i.e if the input size is 1G, it will use 4 reducers.</description>
  </property>
  <property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
    <description>location of default database for the warehouse</description>
  </property>
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://{{HADOOP_MASTER}}:9083</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.apache.derby.jdbc.ClientDriver</value>
    <description>Driver class name for a JDBC metastore</description>
  </property>
 <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby://{{HADOOP_MASTER}}:1527/{{HIVE_METASTORE_DIR}}/metastore_db;create=true</value>
    <description>JDBC connect string for a JDBC metastore</description>
  </property>
  <property>
    <name>hive.map.aggr</name>
    <value>true</value>
    <description>Whether to use map-side aggregation in Hive Group By queries</description>
  </property>
  <property>
    <name>hive.cbo.enable</name>
    <value>true</value>
    <description>Flag to control enabling Cost Based Optimizations using Calcite framework.</description>
  </property>
  <property>
    <name>hive.smbjoin.cache.rows</name>
    <value>10000</value>
    <description>How many rows with the same key value should be cached in memory per smb joined table.</description>
  </property>
  <property>
    <name>hive.map.aggr.hash.percentmemory</name>
    <value>0.5</value>
    <description>Portion of total memory to be used by map-side group aggregation hash table</description>
  </property>
  <property>
    <name>hive.merge.mapfiles</name>
    <value>true</value>
    <description>Merge small files at the end of a map-only job</description>
  </property>
  <property>
    <name>hive.merge.sparkfiles</name>
    <value>false</value>
    <description>Merge small files at the end of a Spark DAG Transformation</description>
  </property>
  <property>
    <name>hive.merge.size.per.task</name>
    <value>256000000</value>
    <description>Size of merged files at the end of the job</description>
  </property>
  <property>
    <name>hive.merge.smallfiles.avgsize</name>
    <value>16000000</value>
    <description>
      When the average output file size of a job is less than this number, Hive will start an additional 
      map-reduce job to merge the output files into bigger files. This is only done for map-only jobs 
      if hive.merge.mapfiles is true, and for map-reduce jobs if hive.merge.mapredfiles is true.
    </description>
  </property>
  <property>
    <name>hive.merge.orcfile.stripe.level</name>
    <value>true</value>
    <description>
      When hive.merge.mapfiles, hive.merge.mapredfiles or hive.merge.tezfiles is enabled
      while writing a table with ORC file format, enabling this config will do stripe-level
      fast merge for small ORC files. Note that enabling this config will not honor the
      padding tolerance config (hive.exec.orc.block.padding.tolerance).
    </description>
  </property>
  <property>
    <name>hive.exec.orc.default.stripe.size</name>
    <value>67108864</value>
    <description>Define the default ORC stripe size, in bytes.</description>
  </property>
  <property>
    <name>hive.orc.splits.include.file.footer</name>
    <value>false</value>
    <description>
      If turned on splits generated by orc will include metadata about the stripes in the file. This
      data is read remotely (from the client or HS2 machine) and sent to all the tasks.
    </description>
  </property>
  <property>
    <name>hive.auto.convert.join</name>
    <value>true</value>
    <description>Whether Hive enables the optimization about converting common join into mapjoin based on the input file size</description>
  </property>
  <property>
    <name>hive.auto.convert.join.noconditionaltask</name>
    <value>true</value>
    <description>
      Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. 
      If this parameter is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the
      specified size, the join is directly converted to a mapjoin (there is no conditional task).
    </description>
  </property>
  <property>
    <name>hive.auto.convert.join.noconditionaltask.size</name>
    <value>894435328</value>
    <description>
      If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. 
      However, if it is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than this size, 
      the join is directly converted to a mapjoin(there is no conditional task). The default is 10MB
    </description>
  </property>
  <property>
    <name>hive.limit.pushdown.memory.usage</name>
    <value>0.4</value>
    <description>The max memory to be used for hash in RS operator for top K selection.</description>
  </property>
  <property>
    <name>hive.optimize.index.filter</name>
    <value>true</value>
    <description>Whether to enable automatic use of indexes</description>
  </property>
  <property>
    <name>hive.optimize.bucketmapjoin.sortedmerge</name>
    <value>false</value>
    <description>Whether to try sorted bucket merge map join</description>
  </property>
  <property>
    <name>hive.optimize.reducededuplication</name>
    <value>true</value>
    <description>
      Remove extra map-reduce jobs if the data is already clustered by the same key which needs to be used again. 
      This should always be set to true. Since it is a new feature, it has been made configurable.
    </description>
  </property>
  <property>
    <name>hive.optimize.reducededuplication.min.reducer</name>
    <value>4</value>
    <description>
      Reduce deduplication merges two RSs by moving key/parts/reducer-num of the child RS to parent RS. 
      That means if reducer-num of the child RS is fixed (order by or forced bucketing) and small, it can make very slow, single MR.
      The optimization will be automatically disabled if number of reducers would be less than specified value.
    </description>
  </property>
  <property>
    <name>hive.optimize.sort.dynamic.partition</name>
    <value>false</value>
    <description>
      When enabled dynamic partitioning column will be globally sorted.
      This way we can keep only one record writer open for each partition value
      in the reducer thereby reducing the memory pressure on reducers.
    </description>
  </property>
  <property>
    <name>hive.stats.autogather</name>
    <value>true</value>
    <description>A flag to gather statistics automatically during the INSERT OVERWRITE command.</description>
  </property>
  <property>
    <name>hive.stats.fetch.column.stats</name>
    <value>true</value>
    <description>
      Annotation of operator tree with statistics information requires column statistics.
      Column statistics are fetched from metastore. Fetching column statistics for each needed column
      can be expensive when the number of columns is high. This flag can be used to disable fetching
      of column statistics from metastore.
    </description>
  </property>
  <property>
    <name>hive.fetch.task.conversion</name>
    <value>more</value>
    <description>
      Expects one of [none, minimal, more].
      Some select queries can be converted to single FETCH task minimizing latency.
      Currently the query should be single sourced not having any subquery and should not have
      any aggregations or distincts (which incurs RS), lateral views and joins.
      0. none : disable hive.fetch.task.conversion
      1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only
      2. more    : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns)
    </description>
  </property>
  <property>
    <name>hive.fetch.task.conversion.threshold</name>
    <value>1073741824</value>
    <description>
      Input threshold for applying hive.fetch.task.conversion. If target table is native, input length
      is calculated by summation of file lengths. If it's not native, storage handler for the table
      can optionally implement org.apache.hadoop.hive.ql.metadata.InputEstimator interface.
    </description>
  </property>
  <property>
    <name>hive.fetch.task.aggr</name>
    <value>false</value>
    <description>
      Aggregation queries with no group-by clause (for example, select count(*) from src) execute
      final aggregations in single reduce task. If this is set true, Hive delegates final aggregation
      stage to fetch task, possibly decreasing the query time.
    </description>
  </property>
  <property>
    <name>hive.compute.query.using.stats</name>
    <value>true</value>
    <description>
      When set to true Hive will answer a few queries like count(1) purely using stats
      stored in metastore. For basic stats collection turn on the config hive.stats.autogather to true.
      For more advanced stats collection need to run analyze table queries.
    </description>
  </property>
  <property>
    <name>hive.vectorized.execution.reduce.enabled</name>
    <value>false</value>
    <description>
      This flag should be set to true to enable vectorized mode of the reduce-side of query execution.
      The default value is true.
    </description>
  </property>
  <property>
    <name>hive.vectorized.groupby.checkinterval</name>
    <value>4096</value>
    <description>Number of entries added to the group by aggregation hash before a recomputation of average entry size is performed.</description>
  </property>
  <property>
    <name>hive.vectorized.groupby.flush.percent</name>
    <value>0.1</value>
    <description>Percent of entries in the group by aggregation hash flushed when the memory threshold is exceeded.</description>
  </property>
  <property>
  <name>mapreduce.input.fileinputformat.list-status.num-threads</name>
  <value>5</value>
  </property>
  <property>
  <name>spark.kryo.referenceTracking</name>
  <value>false</value>
  </property>
  <property>
  <name>spark.kryo.classesToRegister</name>
  <value>org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch</value>
  </property>
</configuration>
